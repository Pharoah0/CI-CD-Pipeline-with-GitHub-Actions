{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy boto3 faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "from faker import Faker\n",
    "import boto3\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Upload dataset to S3\n",
    "bucket_name = \"my_s3_bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Ad Campaign Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Faker and define platforms\n",
    "fake = Faker()\n",
    "campaigns = [\"Facebook\", \"Google Ads\", \"LinkedIn\", \"TikTok\", \"Twitter\", \"Gooogle Ads\", \"Facebok\", \"Tik-Tok\"]  # With typos\n",
    "\n",
    "# Set parameters\n",
    "num_rows = 10_000_000  \n",
    "batch_size = 1_000_000  # Process in batches\n",
    "chunk_size = 50_000     # Write in 50K-row chunks to CSV\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f\"ad_campaign_data_{timestamp}.csv\"  # Unique file name per run\n",
    "output_file = f\"/content/{file_name}\"  # Save locally in Colab\n",
    "\n",
    "# Generate & Save Data in Batches\n",
    "for batch in range(num_rows // batch_size):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        data.append({\n",
    "            \"campaign_id\": fake.uuid4() if random.random() > 0.01 else np.nan,  # 1% missing\n",
    "            \"platform\": random.choice(campaigns),  # Includes typos\n",
    "            \"date\": fake.date_this_year() if random.random() > 0.02 else np.nan,  # 2% missing dates\n",
    "            \"impressions\": random.randint(1000, 50000),\n",
    "            \"clicks\": random.randint(100, 5000) if random.random() > 0.05 else np.nan,  # 5% missing\n",
    "            \"spend\": round(random.uniform(10, 500), 2),\n",
    "            \"conversions\": random.randint(1, 500) if random.random() > 0.05 else np.nan,  # 5% missing\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Introduce Duplicates (2% of each batch)\n",
    "    duplicate_rows = df.sample(frac=0.02, random_state=42)\n",
    "    df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "    # Write Data in Chunks to CSV\n",
    "    for i, chunk in enumerate(range(0, len(df), chunk_size)):\n",
    "        df.iloc[chunk:chunk + chunk_size].to_csv(output_file, mode='a', index=False, header=(batch == 0 and i == 0))\n",
    "\n",
    "    print(f\"Batch {batch + 1} processed and written to {output_file}\")\n",
    "\n",
    "# Upload Dataset to S3\n",
    "s3_key = f\"raw/{file_name}\"  # Store in raw folder\n",
    "s3.upload_file(output_file, bucket_name, s3_key)\n",
    "print(f\"File uploaded to S3: s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "# Optional: Remove local file after upload to save storage\n",
    "os.remove(output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
